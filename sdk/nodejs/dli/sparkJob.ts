// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";
import * as utilities from "../utilities";

export class SparkJob extends pulumi.CustomResource {
    /**
     * Get an existing SparkJob resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: SparkJobState, opts?: pulumi.CustomResourceOptions): SparkJob {
        return new SparkJob(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'cloudru:Dli/sparkJob:SparkJob';

    /**
     * Returns true if the given object is an instance of SparkJob.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is SparkJob {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === SparkJob.__pulumiType;
    }

    declare public readonly appName: pulumi.Output<string>;
    declare public readonly appParameters: pulumi.Output<string[] | undefined>;
    declare public readonly configurations: pulumi.Output<{[key: string]: string} | undefined>;
    declare public /*out*/ readonly createdAt: pulumi.Output<string>;
    declare public readonly dependentPackages: pulumi.Output<outputs.Dli.SparkJobDependentPackage[] | undefined>;
    declare public readonly driverCores: pulumi.Output<number | undefined>;
    declare public readonly driverMemory: pulumi.Output<string | undefined>;
    declare public readonly executorCores: pulumi.Output<number | undefined>;
    declare public readonly executorMemory: pulumi.Output<string | undefined>;
    declare public readonly executors: pulumi.Output<number | undefined>;
    declare public readonly feature: pulumi.Output<string | undefined>;
    declare public readonly files: pulumi.Output<string[] | undefined>;
    declare public readonly jars: pulumi.Output<string[] | undefined>;
    declare public readonly mainClass: pulumi.Output<string | undefined>;
    declare public readonly maxRetries: pulumi.Output<number | undefined>;
    declare public readonly modules: pulumi.Output<string[] | undefined>;
    declare public readonly name: pulumi.Output<string>;
    declare public /*out*/ readonly owner: pulumi.Output<string>;
    declare public readonly pythonFiles: pulumi.Output<string[] | undefined>;
    declare public readonly queueName: pulumi.Output<string>;
    declare public readonly region: pulumi.Output<string>;
    declare public readonly sparkVersion: pulumi.Output<string | undefined>;
    declare public readonly specification: pulumi.Output<string | undefined>;

    /**
     * Create a SparkJob resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args: SparkJobArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: SparkJobArgs | SparkJobState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as SparkJobState | undefined;
            resourceInputs["appName"] = state?.appName;
            resourceInputs["appParameters"] = state?.appParameters;
            resourceInputs["configurations"] = state?.configurations;
            resourceInputs["createdAt"] = state?.createdAt;
            resourceInputs["dependentPackages"] = state?.dependentPackages;
            resourceInputs["driverCores"] = state?.driverCores;
            resourceInputs["driverMemory"] = state?.driverMemory;
            resourceInputs["executorCores"] = state?.executorCores;
            resourceInputs["executorMemory"] = state?.executorMemory;
            resourceInputs["executors"] = state?.executors;
            resourceInputs["feature"] = state?.feature;
            resourceInputs["files"] = state?.files;
            resourceInputs["jars"] = state?.jars;
            resourceInputs["mainClass"] = state?.mainClass;
            resourceInputs["maxRetries"] = state?.maxRetries;
            resourceInputs["modules"] = state?.modules;
            resourceInputs["name"] = state?.name;
            resourceInputs["owner"] = state?.owner;
            resourceInputs["pythonFiles"] = state?.pythonFiles;
            resourceInputs["queueName"] = state?.queueName;
            resourceInputs["region"] = state?.region;
            resourceInputs["sparkVersion"] = state?.sparkVersion;
            resourceInputs["specification"] = state?.specification;
        } else {
            const args = argsOrState as SparkJobArgs | undefined;
            if (args?.appName === undefined && !opts.urn) {
                throw new Error("Missing required property 'appName'");
            }
            if (args?.queueName === undefined && !opts.urn) {
                throw new Error("Missing required property 'queueName'");
            }
            resourceInputs["appName"] = args?.appName;
            resourceInputs["appParameters"] = args?.appParameters;
            resourceInputs["configurations"] = args?.configurations;
            resourceInputs["dependentPackages"] = args?.dependentPackages;
            resourceInputs["driverCores"] = args?.driverCores;
            resourceInputs["driverMemory"] = args?.driverMemory;
            resourceInputs["executorCores"] = args?.executorCores;
            resourceInputs["executorMemory"] = args?.executorMemory;
            resourceInputs["executors"] = args?.executors;
            resourceInputs["feature"] = args?.feature;
            resourceInputs["files"] = args?.files;
            resourceInputs["jars"] = args?.jars;
            resourceInputs["mainClass"] = args?.mainClass;
            resourceInputs["maxRetries"] = args?.maxRetries;
            resourceInputs["modules"] = args?.modules;
            resourceInputs["name"] = args?.name;
            resourceInputs["pythonFiles"] = args?.pythonFiles;
            resourceInputs["queueName"] = args?.queueName;
            resourceInputs["region"] = args?.region;
            resourceInputs["sparkVersion"] = args?.sparkVersion;
            resourceInputs["specification"] = args?.specification;
            resourceInputs["createdAt"] = undefined /*out*/;
            resourceInputs["owner"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(SparkJob.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering SparkJob resources.
 */
export interface SparkJobState {
    appName?: pulumi.Input<string>;
    appParameters?: pulumi.Input<pulumi.Input<string>[]>;
    configurations?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    createdAt?: pulumi.Input<string>;
    dependentPackages?: pulumi.Input<pulumi.Input<inputs.Dli.SparkJobDependentPackage>[]>;
    driverCores?: pulumi.Input<number>;
    driverMemory?: pulumi.Input<string>;
    executorCores?: pulumi.Input<number>;
    executorMemory?: pulumi.Input<string>;
    executors?: pulumi.Input<number>;
    feature?: pulumi.Input<string>;
    files?: pulumi.Input<pulumi.Input<string>[]>;
    jars?: pulumi.Input<pulumi.Input<string>[]>;
    mainClass?: pulumi.Input<string>;
    maxRetries?: pulumi.Input<number>;
    modules?: pulumi.Input<pulumi.Input<string>[]>;
    name?: pulumi.Input<string>;
    owner?: pulumi.Input<string>;
    pythonFiles?: pulumi.Input<pulumi.Input<string>[]>;
    queueName?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
    sparkVersion?: pulumi.Input<string>;
    specification?: pulumi.Input<string>;
}

/**
 * The set of arguments for constructing a SparkJob resource.
 */
export interface SparkJobArgs {
    appName: pulumi.Input<string>;
    appParameters?: pulumi.Input<pulumi.Input<string>[]>;
    configurations?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    dependentPackages?: pulumi.Input<pulumi.Input<inputs.Dli.SparkJobDependentPackage>[]>;
    driverCores?: pulumi.Input<number>;
    driverMemory?: pulumi.Input<string>;
    executorCores?: pulumi.Input<number>;
    executorMemory?: pulumi.Input<string>;
    executors?: pulumi.Input<number>;
    feature?: pulumi.Input<string>;
    files?: pulumi.Input<pulumi.Input<string>[]>;
    jars?: pulumi.Input<pulumi.Input<string>[]>;
    mainClass?: pulumi.Input<string>;
    maxRetries?: pulumi.Input<number>;
    modules?: pulumi.Input<pulumi.Input<string>[]>;
    name?: pulumi.Input<string>;
    pythonFiles?: pulumi.Input<pulumi.Input<string>[]>;
    queueName: pulumi.Input<string>;
    region?: pulumi.Input<string>;
    sparkVersion?: pulumi.Input<string>;
    specification?: pulumi.Input<string>;
}
